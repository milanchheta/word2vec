{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment9.2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtd7sbLFOTe-"
      },
      "source": [
        "# ***Assignment 9 - word2vec***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r21DQXOWC5lO"
      },
      "source": [
        "#import statements\n",
        "import string\n",
        "from sklearn.preprocessing import normalize"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "J2HPvmwKC7J3",
        "outputId": "97270cf5-c5f8-412a-c8f7-115798fc7a7d"
      },
      "source": [
        "# Verify tenseorflow version\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.15.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0QLpaevewu-"
      },
      "source": [
        "# Word pre processing function\n",
        "def pre_process(word):\n",
        "  word = word.lower().strip()\n",
        "  for punc in string.punctuation:\n",
        "    word = word.replace(punc,\"\")\n",
        "  return word"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLOlmRhQC9TN"
      },
      "source": [
        " # input processing function to generate word pairs\n",
        " def process_input(filepath): \n",
        "\n",
        "  #initialize\n",
        "  sentences = open(filepath, 'r').readlines()\n",
        "  size_of_window = 3\n",
        "  word_pairs = []\n",
        "  sentence_list = []\n",
        "\n",
        "  # process input\n",
        "  for sentence in sentences:\n",
        "    sentence_list = [pre_process(word) for word in sentence.split()]\n",
        "    for i, target in enumerate(sentence_list):\n",
        "      for j in range(1,4):\n",
        "        if i+j < len(sentence_list):\n",
        "          word_pairs.append((target,sentence_list[i+j]))\n",
        "        if i-j>= 0:\n",
        "          word_pairs.append((target,sentence_list[i-j]))\n",
        "\n",
        "  # generate word to id, id to word and word pairs\n",
        "  IdToWord = list(set(pair[0] for pair in word_pairs))\n",
        "  wordToId = {w:i for i,w in enumerate(IdToWord)}\n",
        "  word_pairs = [(wordToId[pair[0]],wordToId[pair[1]]) for pair in word_pairs]\n",
        "\n",
        "  return word_pairs, IdToWord ,wordToId\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "793CCVioC_Km"
      },
      "source": [
        "# Function to train the model \n",
        "def train(word_pairs, IdToWord ,wordToId):\n",
        "\n",
        "  # Initialize\n",
        "  vocubSize = len(wordToId)\n",
        "  embeddingSize = 50\n",
        "  batchSize = 1024\n",
        "\n",
        "  # weights and embeddings\n",
        "  w_embed = tf.Variable(tf.random_uniform([vocubSize,embeddingSize]))\n",
        "  weights = tf.Variable(tf.random_uniform([embeddingSize,vocubSize]))\n",
        "\n",
        "  #input and output data\n",
        "  batch = tf.placeholder(tf.int32)\n",
        "  x_train = batch[:,0]\n",
        "  y_train = batch[:,1]\n",
        "\n",
        "  # prediction\n",
        "  wembed = tf.nn.embedding_lookup(w_embed,x_train)\n",
        "  prediction = tf.matmul(wembed,weights)\n",
        "\n",
        "  #optimizer\n",
        "  loss = tf.losses.sparse_softmax_cross_entropy(y_train, prediction)\n",
        "  optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
        "\n",
        "  init=tf.global_variables_initializer()\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "\n",
        "    #Training loop\n",
        "    for i in range(0, len(word_pairs), batchSize):\n",
        "      batchData = word_pairs[i : i + batchSize]\n",
        "      _,lossValue = sess.run([optimizer,loss],feed_dict={batch : batchData})\n",
        "\n",
        "    W = sess.run(w_embed)\n",
        "    return W"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGkqdbiWDAxZ"
      },
      "source": [
        "# Function to get word top `k` similarities\n",
        "def similarity(word, word_to_id, id_to_word, W_, k):\n",
        "  vec = W_[word_to_id[word.lower()]]\n",
        "  sim = np.dot(W_,vec)\n",
        "  sim_idx=np.argsort(sim)[::-1]\n",
        "  res = []\n",
        "  for idx in sim_idx[1:k+1]:\n",
        "    res.append((id_to_word[idx],sim[idx]))\n",
        "  return res"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9VMVSBCDDLT"
      },
      "source": [
        "# Training model for a text file fetched from gutenberg files\n",
        "filepath_gutenberg = './gutenberg.txt'\n",
        "word_pairs_guten, IdToWord_guten, wordToId_guten = process_input(filepath_gutenberg)\n",
        "W_guten = train(word_pairs_guten, IdToWord_guten ,wordToId_guten)\n",
        "np.save('W_gutenberg.npy',W_guten)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2k-yk2dDFE9",
        "outputId": "51521b77-1cb8-4988-808d-d727a8b4e844"
      },
      "source": [
        "# Running the trained model for a text file fetched from gutenberg files\n",
        "W_gutenberg = np.load(\"W_gutenberg.npy\")\n",
        "W_gutenberg = normalize(W_gutenberg)\n",
        "k = 6\n",
        "word=\"compliant\"\n",
        "print(\"Word => {}\\nWord similarities => {}\\n\".format(word, similarity(word, wordToId_guten, IdToWord_guten, W_gutenberg, k)))\n",
        "\n",
        "word=\"unattempted\"\n",
        "print(\"Word => {}\\nWord similarities => {}\\n\".format(word, similarity(word, wordToId_guten, IdToWord_guten, W_gutenberg, k)))\n",
        "\n",
        "word=\"glorious\"\n",
        "print(\"Word => {}\\nWord similarities => {}\\n\".format(word, similarity(word, wordToId_guten, IdToWord_guten, W_gutenberg, k)))\n",
        "\n",
        "word=\"cash\"\n",
        "print(\"Word => {}\\nWord similarities => {}\\n\".format(word, similarity(word, wordToId_guten, IdToWord_guten, W_gutenberg, k)))\n",
        "\n",
        "word=\"stripes\"\n",
        "print(\"Word => {}\\nWord similarities => {}\\n\".format(word, similarity(word, wordToId_guten, IdToWord_guten, W_gutenberg, k)))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word => compliant\n",
            "Word similarities => [('engendering', 0.86162275), ('including', 0.85579246), ('syrtis', 0.85484564), ('assures', 0.8493416), ('bitterness', 0.8468277), ('hospitable', 0.84678)]\n",
            "\n",
            "Word => unattempted\n",
            "Word similarities => [('withstands', 0.8788162), ('confer', 0.8664285), ('altern', 0.8623103), ('venturous', 0.85828805), ('springtime', 0.8579355), ('sworn', 0.85767287)]\n",
            "\n",
            "Word => glorious\n",
            "Word similarities => [('willinger', 0.84164786), ('encroached', 0.83873343), ('tookst', 0.8321131), ('timbrels', 0.8312454), ('adversaryserpent', 0.8309327), ('trance', 0.8292624)]\n",
            "\n",
            "Word => cash\n",
            "Word similarities => [('frisking', 0.87216794), ('whateer', 0.86321104), ('him', 0.83330226), ('hope', 0.8330941), ('he', 0.82983404), ('offerings', 0.82980025)]\n",
            "\n",
            "Word => stripes\n",
            "Word similarities => [('etext', 0.88781816), ('needest', 0.8673834), ('who', 0.86489224), ('anchors', 0.86488086), ('rebuke', 0.85766), ('swage', 0.8573093)]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}